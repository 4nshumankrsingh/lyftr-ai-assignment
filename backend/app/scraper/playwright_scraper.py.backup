"""
Enhanced Playwright scraper with proper interaction serialization
"""
import asyncio
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from bs4 import BeautifulSoup

try:
    from playwright.async_api import async_playwright, Page, TimeoutError as PlaywrightTimeoutError
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logging.warning("Playwright not installed. JS rendering will not work.")

from .base_scraper import BaseScraper
from .static_scraper import StaticScraper
from .interaction_handler import InteractionHandler
from ..models.schemas import ScrapeResult, Meta, Section, Content, Link, Image, Interaction, Error

logger = logging.getLogger(__name__)

class PlaywrightScraper(BaseScraper):
    """Enhanced scraper for JavaScript-rendered websites"""
    
    def __init__(self, headless: bool = True):
        super().__init__()
        self.headless = headless
        self.timeout = 30000
        self.interaction_handler = InteractionHandler(max_depth=3)
        self.static_scraper = StaticScraper()
        
        # Noise selectors
        self.noise_selectors = [
            'script', 'style', 'noscript', 'iframe',
            '[class*="cookie"]', '[class*="banner"]', '[class*="popup"]',
            '[class*="modal"]', '[class*="advertisement"]', '[class*="ad-"]',
            '[id*="cookie"]', '[id*="banner"]', '[id*="popup"]',
            '[role="alert"]', '[aria-label*="cookie"]',
            '.ad-container', '.adsbygoogle', '.ad-slot',
            '.newsletter', '.subscribe-modal'
        ]
    
    async def scrape(self, url: str) -> ScrapeResult:
        """Main scraping method with proper interaction serialization"""
        if not PLAYWRIGHT_AVAILABLE:
            error_msg = "Playwright not installed. Cannot perform JS rendering."
            logger.error(error_msg)
            return self._create_error_result(url, error_msg)
        
        self.url = url
        self.base_url = self._get_base_url(url)
        self.errors = []
        
        start_time = time.time()
        
        try:
            async with async_playwright() as p:
                # Launch browser
                browser = await p.chromium.launch(
                    headless=self.headless,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox'
                    ],
                    timeout=60000
                )
                
                # Create context
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                
                # Create page
                page = await context.new_page()
                page.set_default_timeout(self.timeout)
                
                # Navigate to URL
                logger.info(f"Navigating to {url}")
                try:
                    await page.goto(url, wait_until='domcontentloaded', timeout=self.timeout)
                except Exception as e:
                    logger.warning(f"Initial navigation failed: {e}, trying load")
                    await page.goto(url, wait_until='load', timeout=self.timeout)
                
                # Wait for content
                await self._wait_for_content(page)
                
                # Get initial URL
                current_url = page.url
                visited_pages = [current_url]
                
                # REMOVE ALL NUCLEAR FIX CODE - Keep it clean
                
                # Check if this is Hacker News
                is_hacker_news = 'news.ycombinator.com' in url or 'hacker-news.com' in url
                
                # Initialize interaction tracking
                clicks = []
                scrolls = 0
                pages_visited = [current_url]
                
                # Perform interactions based on site type
                if is_hacker_news:
                    logger.info("ðŸ” HACKER NEWS DETECTED - Performing specialized interactions")
                    
                    # Force scroll first
                    for i in range(3):
                        await page.evaluate(f'window.scrollBy(0, {500 * (i + 1)})')
                        scrolls += 1
                        clicks.append(f"hackernews-scroll:{i+1}")
                        await asyncio.sleep(1.5)
                    
                    # Try to click "More" links
                    for attempt in range(3):
                        try:
                            # Look for .morelink or "More" links
                            more_link = await page.query_selector('.morelink, a:has-text("More")')
                            if more_link and await more_link.is_visible():
                                await more_link.click(timeout=5000)
                                clicks.append(f"hackernews-morelink:click-{attempt+1}")
                                await page.wait_for_load_state('networkidle', timeout=10000)
                                await asyncio.sleep(2)
                                
                                # Update current URL
                                new_url = page.url
                                if new_url not in pages_visited:
                                    pages_visited.append(new_url)
                            else:
                                break
                        except Exception as e:
                            logger.debug(f"More link click {attempt} failed: {e}")
                            break
                else:
                    # Generic interaction handling for other sites
                    logger.info("Starting generic interactive scraping")
                    
                    # Force minimum scrolls
                    for i in range(2):
                        await page.evaluate(f'window.scrollBy(0, {600 * (i + 1)})')
                        scrolls += 1
                        clicks.append(f"scroll:{i+1}")
                        await asyncio.sleep(1)
                    
                    # Try to find interactive elements
                    interactive_result = await self._find_interactive_elements(page)
                    clicks.extend(interactive_result.get('clicks', []))
                    
                    # Update visited pages if navigation happened
                    if page.url != current_url and page.url not in pages_visited:
                        pages_visited.append(page.url)
                
                # Get final HTML after interactions
                final_html = await page.content()
                
                # Close browser
                await browser.close()
                
                # Parse HTML with BeautifulSoup
                soup = BeautifulSoup(final_html, 'lxml')
                
                # Remove noise
                self._remove_noise_from_soup(soup)
                
                # Extract metadata
                metadata = self._extract_metadata(soup)
                
                # Extract sections
                self.static_scraper.url = url
                self.static_scraper.base_url = self.base_url
                sections = self.static_scraper._extract_sections(soup)
                
                # Calculate elapsed time
                elapsed_time = (time.time() - start_time) * 1000
                
                # Create interactions object
                interactions = Interaction(
                    clicks=clicks,
                    scrolls=scrolls,
                    pages=pages_visited,
                    totalDepth=len(clicks) + scrolls
                )
                
                # Debug log
                logger.info(f"âœ… Scraping completed: {len(clicks)} clicks, {scrolls} scrolls, {len(pages_visited)} pages")
                
                return ScrapeResult(
                    url=url,
                    scrapedAt=datetime.utcnow().isoformat() + "Z",
                    meta=Meta(**metadata),
                    sections=sections,
                    interactions=interactions,
                    errors=[Error(message=e["message"], phase=e["phase"]) for e in self.errors],
                    performance={
                        "duration_ms": elapsed_time,
                        "sections_found": len(sections),
                        "interaction_depth": len(clicks) + scrolls,
                        "pages_visited": len(pages_visited)
                    }
                )
                
        except Exception as e:
            logger.error(f"Playwright scraping error: {e}")
            self.add_error(str(e), "js_render")
            return self._create_error_result(url, str(e))
    
    async def _wait_for_content(self, page: Page):
        """Wait for dynamic content to load"""
        try:
            # Wait for network idle
            await page.wait_for_load_state('networkidle', timeout=10000)
        except:
            try:
                # Fallback to load state
                await page.wait_for_load_state('load', timeout=10000)
            except:
                pass
        
        # Additional wait for dynamic content
        await page.wait_for_timeout(2000)
    
    async def _find_interactive_elements(self, page: Page) -> Dict[str, Any]:
        """Find and interact with interactive elements"""
        clicks = []
        
        try:
            # Try load more buttons
            load_more_selectors = [
                'button:has-text("Load more")',
                'button:has-text("Show more")',
                'button:has-text("More")',
                '[class*="load-more"]',
                '[class*="show-more"]'
            ]
            
            for selector in load_more_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        button = elements[0]
                        if await button.is_visible():
                            await button.click(timeout=3000)
                            clicks.append(f"load-more:{selector}")
                            await asyncio.sleep(2)
                            break
                except:
                    continue
            
            # Try tabs
            tab_selectors = [
                '[role="tab"]',
                '.tab-button',
                'button[aria-controls]'
            ]
            
            for selector in tab_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        tab = elements[0]
                        if await tab.is_visible():
                            await tab.click(timeout=3000)
                            clicks.append(f"tab:{selector}")
                            await asyncio.sleep(2)
                            break
                except:
                    continue
                    
        except Exception as e:
            logger.debug(f"Interactive element finding failed: {e}")
        
        return {"clicks": clicks}
    
    def _remove_noise_from_soup(self, soup: BeautifulSoup):
        """Remove noise elements from BeautifulSoup"""
        for selector in self.noise_selectors:
            try:
                for element in soup.select(selector):
                    element.decompose()
            except:
                continue
    
    def _extract_metadata(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """Extract metadata from HTML"""
        metadata = {
            "title": "",
            "description": "",
            "language": "en",
            "canonical": None,
            "keywords": [],
            "author": "",
            "viewport": "",
            "themeColor": "",
            "ogType": ""
        }
        
        try:
            # Title
            title_elem = soup.find('title')
            og_title = soup.find('meta', property='og:title')
            
            if og_title and og_title.get('content'):
                metadata["title"] = og_title['content'].strip()
            elif title_elem and title_elem.string:
                metadata["title"] = title_elem.string.strip()
            
            # Description
            desc_selectors = [
                ('meta', {'name': 'description'}),
                ('meta', {'property': 'og:description'}),
                ('meta', {'name': 'twitter:description'})
            ]
            
            for tag, attrs in desc_selectors:
                elem = soup.find(tag, attrs)
                if elem and elem.get('content'):
                    metadata["description"] = elem['content'].strip()
                    break
            
            # Language
            html_tag = soup.find('html')
            if html_tag and html_tag.get('lang'):
                metadata["language"] = html_tag['lang'].split('-')[0]
            
            # Canonical
            canonical = soup.find('link', rel='canonical')
            if canonical and canonical.get('href'):
                metadata["canonical"] = self._make_absolute_url(canonical['href'])
            
        except Exception as e:
            self.add_error(f"Metadata extraction error: {str(e)}", "metadata")
        
        return metadata
    
    def _create_error_result(self, url: str, error_message: str) -> ScrapeResult:
        """Create error result when scraping fails"""
        return ScrapeResult(
            url=url,
            scrapedAt=datetime.utcnow().isoformat() + "Z",
            meta=Meta(),
            sections=[],
            interactions=Interaction(),
            errors=[Error(message=error_message, phase="js_render")],
            performance={
                "duration_ms": 0,
                "sections_found": 0,
                "interaction_depth": 0,
                "pages_visited": 0
            }
        )